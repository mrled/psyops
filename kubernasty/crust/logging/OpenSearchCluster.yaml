# Docs: <https://github.com/opensearch-project/opensearch-k8s-operator/blob/main/docs/userguide/main.md>
apiVersion: opensearch.opster.io/v1
kind: OpenSearchCluster
metadata:
  name: clusterlogs
  namespace: logging
spec:
  general:
    serviceName: clusterlogs
    version: 2.3.0

    # https://github.com/opensearch-project/opensearch-k8s-operator/blob/main/docs/userguide/main.md#deal-with-max-virtual-memory-areas-vmmax_map_count-errors
    setVMMaxMapCount: true

    additionalVolumes:
      - name: kubernasty-ca-root-cert
        path: /etc/kubernasty-ca
        configMap:
          name: kubernasty-ca-root-cert

  dashboards:
    enable: true
    version: 2.3.0
    replicas: 2

    env:
      - name: LDAP_AUTHENTICATOR_USERNAME
        valueFrom: { secretKeyRef: { name: authenticator, key: username } }
      - name: LDAP_AUTHENTICATOR_PASSWORD
        valueFrom: { secretKeyRef: { name: authenticator, key: password } }

    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "2000m"

    additionalConfig:
      opensearch_security.auth.type: proxy
      opensearch.requestHeadersWhitelist: |
        ["x-forwarded-for","remote-user","remote-groups","remote-email","remote-name"]

      http:
        xff:
          enabled: true
          # This is the IP of the Authelia reverse proxy, assigned by metallb IPAddressPool. ?
          internalProxies: "192.168.1.221"
          remoteIpHeader: "x-forwarded-for"

      authc:
        # We must enable internal auth bc OpenSearch Dashboards connects to OpenSearch using the `kibanaserver` internal user
        internal_auth:
          order: 0
          description: "HTTP basic authentication using the internal user database"
          http_enabled: true
          transport_enabled: true
          http_authenticator:
            type: basic
            challenge: false
          authentication_backend:
            type: internal

        proxy_auth_domain:
          order: 1
          description: "Authenticate via Authelia reverse proxy"
          http_enabled: true
          transport_enabled: true
          http_authenticator:
            type: proxy
            config:
              user_header: "Remote-User"
              email_header: "Remote-Email"
              name_header: "Remote-Name"
              roles_header: "Remote-Groups"
              challenge: false
          authentication_backend:
            type: noop

        # <https://opensearch.org/docs/latest/security/authentication-backends/ldap>
        ldap:
          http_enabled: true
          transport_enabled: true
          order: 2
          http_authenticator:
            type: basic
            challenge: false
          authentication_backend:
            type: ldap
            config:
              hosts:
                - dirsrv.directory.svc.cluster.local:3636
              enable_ssl: true
              enable_start_tls: false
              enable_ssl_client_auth: false
              verify_hostnames: true
              pemtrustedcas_filepath: "/etc/kubernasty-ca/ca.crt"

              bind_dn: "${LDAP_AUTHENTICATOR_USERNAME}"
              password: "${LDAP_AUTHENTICATOR_PASSWORD}"

              # In usersearch, {0} is the username
              usersearch: "(&(uid={0})(objectClass=inetOrgPerson))"
              userbase: "ou=people,dc=micahrl,dc=me"
              username_attribute: "uid"

      authz:
        # <https://opensearch.org/docs/latest/security/authentication-backends/ldap>
        # Most of the configuration is identical to authc.ldap above; this is apparently required.
        ldap:
          http_enabled: true
          transport_enabled: true
          authorization_backend:
            type: ldap
            config:
              hosts:
                - dirsrv.directory.svc.cluster.local:3636
              enable_ssl: true
              enable_start_tls: false
              enable_ssl_client_auth: false
              verify_hostnames: true
              pemtrustedcas_filepath: "/etc/kubernasty-ca/ca.crt"

              bind_dn: "${LDAP_AUTHENTICATOR_USERNAME}"
              password: "${LDAP_AUTHENTICATOR_PASSWORD}"

              # In usersearch, {0} is the username
              usersearch: "(&(uid={0})(objectClass=inetOrgPerson))"
              userbase: "ou=people,dc=micahrl,dc=me"
              username_attribute: "uid"

              # In role search, {0} is the DN of the user
              rolesearch: "(&(member={0})(objectClass=groupOfUniqueNames))"
              rolebase: "ou=groups,dc=micahrl,dc=me"
              # We don't need to resolve nested roles because LDAPEnforcer flattens them
              # (only for enforced users/groups, but that's all we care about)
              resolve_nested_roles: false
              rolename: cn

  # On node roles: <https://opensearch.org/docs/latest/tuning-your-cluster/#nodes>
  nodePools:
    # Cluster manager roles
    - component: master
      replicas: 3
      diskSize: "5Gi"
      nodeSelector:
      resources:
        requests:
          memory: "2Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "2000m"
      roles:
        - "cluster_manager"
      persistence:
        pvc:
          storageClass: cephalopodblk-nvme-2rep
          accessModes: ["ReadWriteOnce"]
      # Don't schedule multiple OpenSearch master node instances on the same Kubernetes node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "opensearch.opster.io/component"
                    operator: In
                    values:
                      - master
              topologyKey: "kubernetes.io/hostname"

    # Data storage and search roles
    - component: data
      replicas: 3
      diskSize: "5Gi"
      nodeSelector:
      resources:
        requests:
          memory: "2Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "2000m"
      roles:
        - "data"
      persistence:
        pvc:
          storageClass: cephalopodblk-nvme-2rep
          accessModes: ["ReadWriteOnce"]
      # Don't schedule multiple OpenSearch data node instances on the same Kubernetes node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "opensearch.opster.io/component"
                    operator: In
                    values:
                      - master
              topologyKey: "kubernetes.io/hostname"

  security:
    tls:
      transport:
        generate: false
        perNode: false # Use the same cert for all nodes; requires the DNS name for all nodes to be present
        secret:
          name: clusterlogs-tls-secret

        # List of DNs with node rights, wildcards ok
        # The hostname is defined as <cluster-name>-<nodepool-component>-<index>
        # The DN is set by Cert Manager to be `CN=common-name`
        nodesDn:
          - "CN=clusterlogs.logging.svc.cluster.local"
          - "CN=clusterlogs-master-*.clusterlogs.logging.svc.cluster.local"
          - "CN=clusterlogs-data-*.clusterlogs.logging.svc.cluster.local"

        adminSecret:
          name: clusterlogs-admin-tls-secret

        # The DN is set by Cert Manager to be `CN=common-name`
        adminDn:
          - "CN=clusterlogs-admin.logging.svc.cluster.local"

      # Use the same cert as the transport cert
      http:
        generate: false
        secret:
          name: clusterlogs-tls-secret
