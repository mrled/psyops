# Docs: <https://github.com/opensearch-project/opensearch-k8s-operator/blob/main/docs/userguide/main.md>
apiVersion: opensearch.opster.io/v1
kind: OpenSearchCluster
metadata:
  name: clusterlogs
  namespace: logging
spec:
  general:
    serviceName: clusterlogs
    version: 2.19.1

    # https://github.com/opensearch-project/opensearch-k8s-operator/blob/main/docs/userguide/main.md#deal-with-max-virtual-memory-areas-vmmax_map_count-errors
    setVMMaxMapCount: true

    # Set options in opensearch.yml for all nodes
    additionalConfig:
      # Must be exactly 16 chars, and set the same for all nodes
      plugins.security.compliance.salt: voo4Bu2cah5ahwei

    additionalVolumes:
      - name: kubernasty-ca-root-cert
        # This MUST be mounted inside /usr/share/opensearch,
        # otherwise some security policy will prevent OpenSearch from reading it,
        # could be JVM security manager or some shit, idk.
        path: /usr/share/opensearch/config/kubernasty-ca
        configMap:
          name: kubernasty-ca-root-cert

  dashboards:
    enable: true
    version: 2.19.1
    replicas: 2

    opensearchCredentialsSecret: { name: clusterlogs-dashboards-creds }

    resources:
      requests:
        memory: "512Mi"
        cpu: "200m"
      limits:
        memory: "512Mi"
        cpu: "2000m"

    # Settings here get added in dashboards containers to
    # /usr/share/opensearch-dashboards/config/opensearch_dashboards.yml
    additionalConfig:
      opensearch.requestHeadersWhitelist: |-
        ["x-forwarded-for","remote-user","remote-groups","remote-email","remote-name"]
      opensearch_security.auth.type: proxy
      opensearch_security.proxycache.user_header: "remote-user"
      opensearch_security.proxycache.roles_header: "remote-groups"

  # On node roles: <https://opensearch.org/docs/latest/tuning-your-cluster/#nodes>
  nodePools:
    # Cluster manager roles
    - component: master
      replicas: 3
      diskSize: "50Gi"
      resources:
        requests:
          memory: "4Gi"
          cpu: "100m"
        limits:
          memory: "4Gi"
          cpu: "4000m"
      # JVM options:
      # -Xmx and -Xms: Default to half of the memory request
      # -Xx:-UseContainerSupport: disable JDK cgroup probe
      #     This is necessary in Alpine 3.20 or 3.21,
      #     where without it OpenSearch fails to start with
      #     java.lang.NullPointerException: Cannot invoke "jdk.internal.platform.CgroupInfo.getMountPoint()" because "anyController" is null
      #     It means the cgroupv2 controller isn't mounted in the container.
      #     This might be fixed in future versions of OpenSearch (2.3.1?) and then we can remove this.
      jvm: -Xmx2048m -Xms2048m
      roles:
        - "cluster_manager"
      persistence:
        pvc:
          storageClass: cephalopodblk-nvme-2rep
          accessModes: ["ReadWriteOnce"]
      # Don't schedule multiple OpenSearch master node instances on the same Kubernetes node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "opster.io/opensearch-nodepool"
                    operator: In
                    values: [master]
              topologyKey: "kubernetes.io/hostname"

    # Data storage and search roles
    - component: data
      replicas: 3
      diskSize: "50Gi"
      resources:
        requests:
          memory: "4Gi"
          cpu: "100m"
        limits:
          memory: "4Gi"
          cpu: "4000m"
      jvm: -Xmx2048m -Xms2048m
      roles:
        - "data"
      persistence:
        pvc:
          storageClass: cephalopodblk-nvme-2rep
          accessModes: ["ReadWriteOnce"]
      # Don't schedule multiple OpenSearch data node instances on the same Kubernetes node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "opster.io/opensearch-nodepool"
                    operator: In
                    values: [data]
              topologyKey: "kubernetes.io/hostname"

  security:
    config:
      # Contains a security.yml configuration file
      # Is automatically applied by a *-securityconfig-update job
      # which picks up changes and reruns within a few minutes of the Secret being changed
      # See its logs if you have issues
      securityConfigSecret: { name: security-config }

      # Contains a cert signed by the same CA as the transport cert
      # for the operator to use for managing the cluster
      # Required because we are providing our own transport cert
      adminSecret: { name: clusterlogs-admin-tls-secret }

      # Contains username/password for admin access
      # Required if securityConfigSecret is set
      adminCredentialsSecret: { name: clusterlogs-admin-creds }

    tls:
      transport:
        generate: false
        perNode: false # Use the same cert for all nodes; requires the DNS name for all nodes to be present
        secret:
          name: clusterlogs-tls-secret

        # List of DNs with node rights, wildcards ok
        # The hostname is defined as <cluster-name>-<nodepool-component>-<index>
        # The DN is set by Cert Manager to be `CN=common-name`
        nodesDn:
          - "CN=clusterlogs.logging.svc.cluster.local"
          - "CN=clusterlogs-master-*.clusterlogs.logging.svc.cluster.local"
          - "CN=clusterlogs-data-*.clusterlogs.logging.svc.cluster.local"

        # The DN is set by Cert Manager to be `CN=common-name`
        # This must be the DN in clusterlogs-admin-tls-secret
        adminDn:
          - "CN=clusterlogs-admin.logging.svc.cluster.local"

      # Use the same cert as the transport cert
      http:
        generate: false
        secret:
          name: clusterlogs-tls-secret
