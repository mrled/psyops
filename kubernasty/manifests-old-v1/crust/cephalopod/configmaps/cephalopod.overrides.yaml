apiVersion: v1
kind: ConfigMap
metadata:
  name: cephalopod-overrides
  namespace: cephalopod
data:
  values.yaml: |-
    operatorNamespace: rook-ceph

    toolbox:
      enabled: true
      resources:
        requests:
          cpu: 100m
          memory: 128Mi

    cephClusterSpec:
      dataDirHostPath: /psyopsos-data/roles/k3s/rook-ceph/data

      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
        port: 8080

      network:
        connections:
          # TODO: do I need encryption once I have an encrypted network fabric for the whole cluster?
          encryption: true

      # This is pretty anemic, but the CPU requirements are so high by default I couldn't run much else
      resources:
        mgr:
          requests:
            cpu: 100m
        mon:
          requests:
            cpu: 100m
        osd:
          requests:
            cpu: 100m
        mgr-sidecar:
          requests:
            cpu: 100m
        crashcollector:
          requests:
            cpu: 100m
        logcollector:
          requests:
            cpu: 100m
        cleanup:
          requests:
            cpu: 100m


      # See:
      # <https://github.com/rook/rook/blob/master/Documentation/CRDs/Cluster/ceph-cluster-crd.md#node-settings>
      # <https://github.com/rook/rook/blob/master/Documentation/CRDs/Cluster/ceph-cluster-crd.md#storage-selection-settings>
      storage:
        useAllNodes: false
        useAllDevices: false
        # WARNING: ONCE THE CLUSTER IS DEPLOYED, CHANGES TO THIS SECTION ARE NOT AUTOMATICALLY APPLIED
        # See /kubernasty/labnotes/content/docs/crust/ceph.md in this repo for details.
        # It would be more performant to have multiple OSDs per device, but they require a lot of CPU
        nodes:
          - name: jesseta
            devices:
              - name: /dev/nvme0n1
                config:
                  encryptedDevice: "true"
                  osdsPerDevice: "1"
          - name: kenasus
            devices:
              - name: /dev/nvme0n1
                config:
                  encryptedDevice: "true"
                  osdsPerDevice: "1"
          - name: zalas
            devices:
              - name: /dev/nvme0n1
                config:
                  encryptedDevice: "true"
                  osdsPerDevice: "1"

      # "!!! caution Changing networking configuration after a Ceph cluster has been deployed is NOT supported and will result in a non-functioning cluster."
      # lol
      network:
        provider: host
        connections:
          encryption:
            enabled: false
          compression:
            enabled: false

    ingress:
      dashboard:
        annotations:
          kubernetes.io/ingress.class: traefik
          cert-manager.io/cluster-issuer: letsencrypt-issuer-prod
          traefik.ingress.kubernetes.io/router.middlewares: kube-system-keycloak-auth-redirect-mw@kubernetescrd
        host:
          name: cephalopod.${clusterTld}
          paths: /(.*)
        tls:
          - hosts: [cephalopod.${clusterTld}]
            secretName: micahrl-dot-me-wildcard-backing-secret

    cephBlockPools:
      - name: cephalopod-nvme-3rep-block
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: cephalopod-nvme-3rep-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true

    cephFileSystems:
      - name: cephalopod-nvme-3rep-fs
        spec:
          metadataPool:
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              replicated:
                size: 3
              name: datapool0
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              requests:
                cpu: 100m
        storageClass:
          enabled: true
          isDefault: false
          name: cephalopod-nvme-3rep-fs
          pool: datapool0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          mountOptions: []


    cephFileSystemVolumeSnapshotClass:
      enabled: true
      name: cephalopod-nvme-3rep-fs
      isDefault: true
      deletionPolicy: Delete

    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: cephalopod-nvme-3rep-block
      isDefault: false
      deletionPolicy: Delete

    cephObjectStores:
      - name: cephalopod-nvme-3rep-object
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
          dataPool:
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          preservePoolsOnDelete: true
          gateway:
            port: 80
            # securePort: 443
            # sslCertificateRef:
            instances: 1
            resouorces:
              requests:
                1pu: 200m
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: cephalopod-nvme-3rep-object
          reclaimPolicy: Delete
          parameters:
            region: us-office-closet-1
