apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: cephalopod
  namespace: cephalopod
spec:

  cephVersion:
    # Container used to launch all the Ceph daemon pods.
    # Use a specific version tag to ensure the same version is used across all pods.
    # See tags available at https://quay.io/repository/ceph/ceph?tab=tags&tag=latest
    image: quay.io/ceph/ceph:v18.2.2

  dataDirHostPath: /psyopsos-data/roles/kubernasty/cephalopod-data-dir

  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false

  # WaitTimeoutForHealthyOSDInMinutes defines the time (in minutes) the operator would wait before an OSD can be stopped for upgrade or restart.
  waitTimeoutForHealthyOSDInMinutes: 10

  # Whether or not requires PGs are clean before an OSD upgrade.
  upgradeOSDRequiresHealthyPGs: false

  # Recommend odd number of mons, 3 in particular, only one per node, for HA
  mon:
    count: 3
    allowMultiplePerNode: false

  # Active/standby HA with count: 2
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      # List of modules to optionally enable or disable.
      # Note the "dashboard" and "monitoring" modules are already configured by other settings in the cluster CR.
      - name: rook
        enabled: true

  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    port: 80
    ssl: false
    # The url of the Prometheus instance
    # prometheusEndpoint: <protocol>://<prometheus-host>:<port>
    # Whether SSL should be verified if the Prometheus server is using https
    # prometheusEndpointSSLVerify: false

  # enable prometheus alerting for cluster - requires Prometheus to be pre-installed
  # TODO: deploy Prometheus and enable monitoring
  monitoring:
    enabled: false
    # Disable metrics altogether (since we aren't using them anyway)
    metricsDisabled: true

  network:
    connections:
      encryption:
        enabled: true
      compression:
        enabled: false
      # Require newer msgr2 protocol for connections
      requireMsgr2: true

  # enable the crash collector for ceph daemon crash collection
  crashCollector:
    disable: false
    daysToRetain: 7

  # enable log collector, daemons will log on files and rotate
  logCollector:
    enabled: true
    periodicity: daily # one of: hourly, daily, weekly, monthly
    maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.

  # How to handle requests to delete the entire cluster
  # <https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/ceph-teardown.md#delete-the-data-on-hosts>
  cleanupPolicy:
    # To enable cluster deletion, set this to "yes-really-destroy-data".
    # Once set, Rook will stop configuring the cluster.
    confirmation: ""
    # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
    sanitizeDisks:
      # Don't securely overwrite disks
      # (Rely on encryption specified elsewhere.)
      method: quick # Just remove Ceph's metadata, don't zero the whole disk
      dataSource: zero # Don't use system entropy for this, what the fuck and lol
      iteration: 1 # More than 1 hasn't been best practice since like the 90s
    # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
    allowUninstallWithVolumes: false

  # This is pretty anemic, but the CPU requirements are so high by default I couldn't run much else
  resources:
    mgr:
      requests:
        cpu: 100m
    mon:
      requests:
        cpu: 100m
    osd:
      requests:
        cpu: 100m
    mgr-sidecar:
      requests:
        cpu: 100m
    crashcollector:
      requests:
        cpu: 100m
    logcollector:
      requests:
        cpu: 100m
    cleanup:
      requests:
        cpu: 100m

  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false

  # Set Kubernetes priority classes for pod types
  priorityClassNames:
    #all: rook-ceph-default-priority-class
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
    #crashcollector: rook-ceph-crashcollector-priority-class

  # Actually configure where the storage comes from
  storage:

    # We'll specify the individual nodes and devices for this cluster
    useAllNodes: true
    useAllDevices: true

    config:
      # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB

      # It would be more performant to have multiple OSDs per device, but they require a lot of CPU
      osdsPerDevice: "1"
      encryptedDevice: "true"

    # Specify node storage; the 'name' field should match the 'kubernetes.io/hostname' label
    nodes:
      - name: jesseta
        devices:
          - name: /dev/nvme0n1
            config:
              encryptedDevice: "true"
              osdsPerDevice: "1"
              osdClass: nvme
          # - name: /dev/sdX
          #   config:
          #     encryptedDevice: "true"
          #     osdsPerDevice: "1"
          #     osdClass: ssd
      - name: kenasus
        devices:
          - name: /dev/nvme0n1
            config:
              encryptedDevice: "true"
              osdsPerDevice: "1"
              osdClass: nvme
      - name: zalas
        devices:
          - name: /dev/nvme0n1
            config:
              encryptedDevice: "true"
              osdsPerDevice: "1"
              osdClass: nvme

    # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
    onlyApplyOSDPlacement: false

    # The ratio at which Ceph should block IO if the OSDs are too full. The default is 0.95.
    # fullRatio: 0.95
    # The ratio at which Ceph should stop backfilling data if the OSDs are too full. The default is 0.90.
    # backfillFullRatio: 0.90
    # The ratio at which Ceph should raise a health warning if the OSDs are almost full. The default is 0.85.
    # nearFullRatio: 0.85

  # The section for configuring management of daemon disruptions during upgrade or fencing.
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  # csi defines CSI Driver settings applied per cluster.
  csi:
    readAffinity:
      # Allow serving reads from an OSD closest to the client
      # according to the CRUSH map (uses more RAM).
      # https://rook.io/docs/rook/latest/Storage-Configuration/Ceph-CSI/ceph-csi-drivers/#enable-read-affinity-for-rbd-volumes
      enabled: true

  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
    startupProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false